<p>{% include JB/setup %}</p>



<h1 id="em算法的整体思路和理解">EM算法的整体思路和理解</h1>

<p>本文主要来自李航老师的《统计学习方法》和网络的参考资料总结而成的自身对EM算法的理解。</p>



<h2 id="常见使用场景">常见使用场景</h2>

<p>EM算法是一种<strong>迭代</strong>算法，主要针对机器学习模型中，包含不可见的隐变量Z的模型的参数求解。</p>



<h2 id="概念解释">概念解释</h2>

<p><strong>观测数据 Y</strong> 指样本可以直接观测到的随机变量的值，如身高，体重等等 <br>
<strong>隐变量 Z</strong>主要指的是隐藏在数据中的随机变量的数据</p>

<p><strong>这里举个例子</strong> 假设一堆随机1W个人，测量他们的身高数据。若样本中存在男性和女性，身高分别服从 <script type="math/tex" id="MathJax-Element-1">N\left(\mu_1, \sigma_1 \right) ,  N\left( \mu_2 , \sigma_2 \right)</script>，估计<script type="math/tex" id="MathJax-Element-2">\mu_1 , \sigma_1 , \mu_2, \sigma_2</script> 的值。 <br>
这里，身高数据即为我们观测到数据Y， 男性和女性为隐变量Z。 <br>
<script type="math/tex" id="MathJax-Element-3">\mu_1 , \sigma_1 , \mu_2, \sigma_2</script> 就是我们要求的模型参数</p>



<h2 id="em算法的整体框架">EM算法的整体框架：</h2>

<p><strong>输入：</strong> 观测变量数据<strong>Y</strong>， 隐藏变量数据<strong>Z</strong>, 联合分布<script type="math/tex" id="MathJax-Element-4"> P \left( Y,Z |\theta \right)</script>,    条件分布<script type="math/tex" id="MathJax-Element-5">P \left( Z|Y, \theta \right)</script>； <br>
<strong>输出：</strong> 模型参数<script type="math/tex" id="MathJax-Element-6">\theta</script> <br>
(1)、选择参数的初值<script type="math/tex" id="MathJax-Element-7">\theta^\left ( 0 \right )</script> ， 这里参数随机选择，开始迭代 <br>
(2)、E步： 记 <script type="math/tex" id="MathJax-Element-8">\theta^\left( i \right)</script>为第i次迭代参数<script type="math/tex" id="MathJax-Element-9">\theta</script>的估计值， 在第 i+1 次迭代的E步，计算</p>



<p><script type="math/tex; mode=display" id="MathJax-Element-10">
\begin{aligned} Q\left( \theta , \theta^\left( i \right) \right) = E_Z [logP(Y,Z|\theta) | Y,\theta^\left( i \right)\end{aligned} 
</script></p>



<p><script type="math/tex; mode=display" id="MathJax-Element-11">
\begin{aligned} \ =\sum_{Z} logP\left( Y,Z\ |\  \theta  \right) P \left( Z\ |\ Y, \theta^\left(i \right)\right)\end{aligned} 
</script></p>

<p>这里 <br>
<script type="math/tex" id="MathJax-Element-12"> P\left(Z | Y, \theta^ \left( i\right)\right) </script> <br>
是在给定观测数据Y和当前的参数估计 <script type="math/tex" id="MathJax-Element-13">\theta^\left( i \right)</script> 下隐变量数据Z的条件概率分布</p>

<p>(3)、M步：求使<script type="math/tex" id="MathJax-Element-14">Q(\theta, \theta^\left( i\right))</script>极大化的<script type="math/tex" id="MathJax-Element-15">\theta</script>,确定第i+1次迭代的参数的估计值<script type="math/tex" id="MathJax-Element-16">\theta ^\left( i+1\right)</script> <br>
<script type="math/tex; mode=display" id="MathJax-Element-17">\theta^\left(i+1\right) = arg \max\limits_\theta \ Q(\theta, \theta^\left( i\right))</script></p>

<p><strong>Q函数：</strong>  <br>
<script type="math/tex; mode=display" id="MathJax-Element-18">
Q(\theta, \theta^\left( i\right)) = E_Z [ln\ P(Y,Z|\theta)| Y, \theta^\left(i\right)]
</script></p>

<p>这里，对整个EM的算法框架介绍完毕。</p>



<h2 id="隐变量z的推导">隐变量Z的推导</h2>

<p><strong>问题:</strong> 为什么隐变量Z是关于观测数据Y和<script type="math/tex" id="MathJax-Element-19">\theta</script>的条件概率分布 <br>
<script type="math/tex" id="MathJax-Element-20">P\left( Z|Y, \theta \right)</script></p>

<p><strong>推导:</strong> 观测数据Y关于参数<script type="math/tex" id="MathJax-Element-21">\theta</script>的对数似然函数：</p>



<p><script type="math/tex; mode=display" id="MathJax-Element-22">
L(\theta) = log\ P(Y|\theta) = \sum \limits_{i=1}^nlog \sum \limits_Z P(y, z | \theta)
</script> </p>



<p><script type="math/tex; mode=display" id="MathJax-Element-23">
= \sum \limits_{i=1}^nlog (\sum \limits_Z P(Y|Z,\theta)P(Z|\theta))
</script></p>

<p><strong>假设:</strong> 假设D是隐变量Z的某一个分布，且D&gt;=0,则有：</p>



<p><script type="math/tex; mode=display" id="MathJax-Element-24">
= \sum \limits_{i=1}^n log (\sum \limits_Z P(Y|Z,\theta)P(Z|\theta))
</script> </p>



<p><script type="math/tex; mode=display" id="MathJax-Element-25">
= \sum \limits_{i=1}^n log\sum \limits_Z D_i(z_i) \frac{P(y_i, z_i | \theta)}{D_i(z_i)}
</script></p>



<p><script type="math/tex; mode=display" id="MathJax-Element-26">
\geq \sum \limits_{i=1}^n \sum \limits_Z D_i(z_i) log \frac{P(y_i, z_i | \theta)}{D_i(z_i)}
</script></p>

<p>这里，我们对<script type="math/tex" id="MathJax-Element-27">L(\theta)</script>取下界，利用Jensen不等式，等号成立条件：</p>



<p><script type="math/tex; mode=display" id="MathJax-Element-28">
\frac {P(y_i,z_i |\theta)} {D_i(z_i)} = c
</script> <br>
即 <script type="math/tex" id="MathJax-Element-29"> D_i \propto P(y_i, z_i |\theta) </script> 且  <script type="math/tex" id="MathJax-Element-30">\sum \limits_{z} D_i(z_i) =1</script></p>

<p><strong>推导出</strong></p>



<p><script type="math/tex; mode=display" id="MathJax-Element-31">
D_i(Z_i) =  \frac {P(y_i, z_i|\theta)} {P(y_i,z|\theta)}
</script></p>



<p><script type="math/tex; mode=display" id="MathJax-Element-32">
=\frac {P(y_i, z_i | \theta)} {P(y_i | \theta)}
</script></p>



<p><script type="math/tex; mode=display" id="MathJax-Element-33">
= P(z_i|y_i, \theta)
</script></p>

<p><strong>得到结论</strong> <br>
隐变量Z是关于y和<script type="math/tex" id="MathJax-Element-34">\theta</script> 的条件概率,<script type="math/tex" id="MathJax-Element-35">D_i(Z)</script>的计算公式就是后验概率。</p>



<h2 id="em算法的导出">EM算法的导出</h2>

<p><strong>验证EM的正确性，取下界是正确的:</strong> <br>
这里，我们仍然取<script type="math/tex" id="MathJax-Element-36">L(\theta)</script>:</p>



<p><script type="math/tex; mode=display" id="MathJax-Element-37">
L(\theta) = log\ P(Y|\theta) = \sum \limits_{i=1}^nlog \sum \limits_Z P(y, z | \theta)
</script> </p>



<p><script type="math/tex; mode=display" id="MathJax-Element-38">
= \sum \limits_{i=1}^nlog (\sum \limits_Z P(Y|Z,\theta)P(Z|\theta))
</script></p>

<p>我们希望通过对<script type="math/tex" id="MathJax-Element-39">L(\theta)</script>的迭代，逐步近似极大化<script type="math/tex" id="MathJax-Element-40">L(\theta)</script>的值，即<script type="math/tex" id="MathJax-Element-41">L(\theta) > L(\theta^t)</script>,为此，我们考虑二者的差值,这里我们隐去对<script type="math/tex" id="MathJax-Element-42">\sum \limits_{i=1}^n</script>：</p>



<p><script type="math/tex; mode=display" id="MathJax-Element-43">
L(\theta) - L(\theta^t) = log (\sum \limits_Z P(Y|Z,\theta)P(Z|\theta)) - logP(Y|\theta^t)
</script> </p>



<p><script type="math/tex; mode=display" id="MathJax-Element-44">
=log(\sum \limits_Z P(Y|Z, \theta^t) \frac {P(Y|Z, \theta) P(Z | \theta))} {P(Y|Z,\theta^t)}) - log P (Y | \theta^t)
</script></p>



<p><script type="math/tex; mode=display" id="MathJax-Element-45">
\geq \sum \limits_Z P (Z|Y, \theta^t) log \frac {P(Y|Z,\theta)P(Z|\theta)} {P(Z|Y,\theta^t)} - logP(Y|\theta^t)
</script></p>



<p><script type="math/tex; mode=display" id="MathJax-Element-46"> = \sum \limits_Z P(Z|Y,\theta^t) log \frac{P(Y|Z,\theta) P(Z|\theta)} {P(Z|Y, \theta^t)P(Y|\theta^t)}
</script></p>

<p>令<script type="math/tex" id="MathJax-Element-47">B(\theta, \theta^t) =L(\theta^t) +  \sum \limits_Z P(Z|Y,\theta^t) log \frac{P(Y|Z,\theta) P(Z|\theta)} {P(Z|Y, \theta^t)P(Y|\theta^t)}</script></p>

<p><strong>则有：</strong></p>



<p><script type="math/tex; mode=display" id="MathJax-Element-48">
L(\theta)  \geq B(\theta, \theta^i)
</script></p>

<p>即<script type="math/tex" id="MathJax-Element-49">B</script>函数是而一个下界，因此，任何以使<script type="math/tex" id="MathJax-Element-50">B</script>增大的<script type="math/tex" id="MathJax-Element-51">\theta</script>，也可以使<script type="math/tex" id="MathJax-Element-52">L</script>增大。 <br>
这里，选择<script type="math/tex" id="MathJax-Element-53">\theta^\left(t+1\right)</script>使<script type="math/tex" id="MathJax-Element-54">B</script>达到最大。</p>



<p><script type="math/tex; mode=display" id="MathJax-Element-55">
theta^\left( i+1\right) = arg \max \limits_\theta l^`(\theta)
</script></p>



<p><script type="math/tex; mode=display" id="MathJax-Element-56">
=arg \max \limits_\theta \sum \limits_{i=1}^n \sum \limits_z Q_i log \frac {P(x,z|\theta)} {Q_i}
</script></p>



<p><script type="math/tex; mode=display" id="MathJax-Element-57">
= arg \max \limits_\theta \sum \limits_{i=1}^n \sum \limits_z p(z|x, \theta_t) log \frac {P(x,z|\theta)} {P(z|x,\theta_t)}
</script></p>



<p><script type="math/tex; mode=display" id="MathJax-Element-58">
=arg \max \limits_\theta \sum \limits_{i=1}^n \sum \limits_z p(z|x, \theta_t) log P(x,z|\theta) - p(z|x, \theta_t) {P(z|x,\theta_t)}
</script></p>

<p>因为 <script type="math/tex" id="MathJax-Element-59">P(z|x, \theta_t) {P(z|x,\theta_t)}</script> 中的  <br>
<script type="math/tex" id="MathJax-Element-60"> \theta_t</script>都是定值，所以在对 <script type="math/tex" id="MathJax-Element-61">\theta</script> 求导的时候，会消掉，所有只有前面这部分 <script type="math/tex" id="MathJax-Element-62">arg \max \limits_\theta \sum \limits_{i=1}^n \sum \limits_z p(z|x, \theta_t) log P(x,z|\theta)</script> </p>



<h2 id="em问题">EM问题</h2>

<p>似然函数对于离散情况是log概率，连续时是log的概率密度，极大似然是说出现data的概率最大，对于连续的情况怎么联系 概率密度最大 就等价概率最大</p>



<h2 id="reference-links">Reference Links</h2>

<p><a href="http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html">http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html</a></p>

<p><a href="http://blog.csdn.net/abcjennifer/article/details/8170378">http://blog.csdn.net/abcjennifer/article/details/8170378</a></p>